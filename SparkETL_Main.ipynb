{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1969d93-83de-4afc-b1fd-33301bffa2c0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime, math \n",
    "import config\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, lower, upper, col, trim, year, month, dayofweek, translate, concat, regexp_replace, date_format, date_add, round\n",
    "from pyspark.sql.types import IntegerType, StringType, IntegerType, DoubleType\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from redShiftFunctions import redShiftQuery, redShiftWrite, redShiftRead\n",
    "\n",
    "\n",
    "# REPLACE ACCESS KEY AND SECRET KEY in config.py file!\n",
    "conf = SparkConf() \\\n",
    "       .set(\"spark.hadoop.fs.s3a.access.key\", config.s3aAccessKey) \\\n",
    "       .set(\"spark.hadoop.fs.s3a.secret.key\", config.s3aSecretKey)\\\n",
    "       .set(\"spark.hadoop.fs.s3a.endpoint\", config.s3aEndpoint)\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder.master(\"local[*]\") \\\n",
    "        .appName(\"Main ETL for the Capstone Project\")\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "#Used to collect all dataframes.\n",
    "tableNamesArray = []\n",
    "tableNamesArray.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d07d6f-9b6d-4f44-8dc4-5011833e8fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#  ------  PORT LOCATION TABLE -----------\n",
    "\n",
    "#Read the Portal Codes json file.\n",
    "textFile = spark.sparkContext.textFile(config.prtlCodes_file)\n",
    "dfPortCodes = spark.read.json(textFile)\n",
    "\n",
    "#Remove any rows that do not contain a name and drop duplicates.\n",
    "dfPortCodes = dfPortCodes.filter(~dfPortCodes.name.contains(\"No PORT\") & ~dfPortCodes.name.contains(\"Collapsed\") ).drop_duplicates()\n",
    "\n",
    "#Split the name to columns municipality and statecode.\n",
    "nameSplit = split(dfPortCodes.name, ',')\n",
    "dfPortCodes = dfPortCodes.withColumn('municipality', trim (lower( nameSplit.getItem(0) ) ) ) \\\n",
    "                         .withColumn('statecode', trim(upper(nameSplit.getItem(1)))  ) \n",
    "\n",
    "#Remove any rows without state code. Drop column name as its been split to two new columns.\n",
    "dfPortCodes = dfPortCodes.filter(~dfPortCodes.statecode.isNull()).drop(\"name\")\n",
    "\n",
    "dfStateCode = spark.read.options(header=True, delimiter=',', inferSchema='True').csv(config.stateCodes_file)\n",
    "\n",
    "#Joins Portal Codes and State Codes together to have full state name.\n",
    "dfPortCodes = dfPortCodes.join(dfStateCode, dfPortCodes.statecode == dfStateCode[\"Alpha code\"], \"left\")\n",
    "\n",
    "#Drops Alpha code which is a duplicate of state code.\n",
    "dfPortCodes = dfPortCodes.drop(\"Alpha code\").withColumn(\"state\", lower(dfPortCodes.State))\n",
    "\n",
    "dfPortCodes.createOrReplaceTempView(\"PORTCODES\")\n",
    "\n",
    "if config.limitDFtoTen == True:\n",
    "    dfPortCodes = dfPortCodes.limit(10)\n",
    "\n",
    "#Appends Data frame with Key to be used at the end to insert into Database.\n",
    "tableNamesArray.append({'PORTCODES':dfPortCodes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa992e-5e15-4168-a605-4cd263c958d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  ------  AIRPORTS TABLE\n",
    "#\n",
    "\n",
    "dfAirPorts = spark.read.option(\"header\",True).csv(config.airportCodes_file)\n",
    "\n",
    "# Our analysis will be set for US only.\n",
    "dfAirPorts = dfAirPorts.filter( (dfAirPorts.iso_country == 'US') & (dfAirPorts.type != 'heliport') & (dfAirPorts.type != 'closed') )\n",
    "latLongSplit = split(dfAirPorts.coordinates, ',')\n",
    "isoRegSplit = split(dfAirPorts.iso_region, '-')\n",
    "\n",
    "dfAirPorts = dfAirPorts.withColumn('longitude', latLongSplit.getItem(0).cast(DoubleType())) \\\n",
    "        .withColumn('latitude', latLongSplit.getItem(1).cast(DoubleType())) \\\n",
    "        .withColumn('statecode', upper(isoRegSplit.getItem(1))) \\\n",
    "        .withColumn('municipality', lower(col(\"municipality\"))) \\\n",
    "        .withColumn('name', lower(col(\"name\"))) \\\n",
    "        .withColumn('elevation_ft', col(\"elevation_ft\").cast(IntegerType())) \n",
    "\n",
    "\n",
    "#Remove columns continent and iso_country since it is US only.\n",
    "dfAirPorts = dfAirPorts.drop(\"iso_region\", \"continent\", \"coordinates\", \"gps_code\", \"local_code\")\n",
    "\n",
    "if config.limitDFtoTen == True:\n",
    "    dfAirPorts = dfAirPorts.limit(10)\n",
    "\n",
    "#Append table.\n",
    "tableNamesArray.append({'AIRPORTS':dfAirPorts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af6b24-6d1e-4be8-8df8-74f032484a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  ------  USATEMP  TABLE\n",
    "#\n",
    "\n",
    "dfTemps = spark.read.options(header=True, delimiter=',', inferSchema='True').csv(config.globalLandTemp_file)\n",
    "\n",
    "#Filter for US only and removes any rows where Average Temp does not exist.\n",
    "dfTemps = dfTemps.filter( (dfTemps.Country == 'United States') & (dfTemps.AverageTemperature != 'Nan') )\n",
    "\n",
    "#Splits date into year, month, and day of week. Lower cases state to make it easy to join.\n",
    "dfTemps = dfTemps.withColumn('year', year(dfTemps.dt)) \\\n",
    "          .withColumn('month', month(dfTemps.dt))\\\n",
    "          .withColumn('dayOfweek', dayofweek(dfTemps.dt))\\\n",
    "          .withColumn('State', lower (dfTemps.State)) \n",
    "\n",
    "#Create an unique ID for the rows. Rounds Tempature to 3 decimel places.\n",
    "dfTemps = dfTemps.withColumn(\"id\", concat(translate(dfTemps[\"dt\"], \"-\" , \"\"), dfTemps.State )) \\\n",
    "                 .withColumn(\"AverageTemperature\", round(\"AverageTemperature\", 3)) \\\n",
    "                 .withColumn(\"AverageTemperatureUncertainty\", round(\"AverageTemperatureUncertainty\", 3))\n",
    "\n",
    "dfTemps = dfTemps.drop(\"dt\", \"Country\")\n",
    "\n",
    "if config.limitDFtoTen == True:\n",
    "    dfTemps = dfTemps.limit(10)\n",
    "    \n",
    "#Append table.\n",
    "tableNamesArray.append({'USATEMP':dfTemps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45c85f-de2c-45fe-a4b2-cca6be8e21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  ------  USADEMOGRAPHICS  TABLE\n",
    "#\n",
    "dfCityDemo = spark.read.options(header=True, delimiter=';').csv(config.usDemograph_file)\n",
    "\n",
    "#Lower cases columns to be used easily when joining.\n",
    "dfCityDemo = dfCityDemo.withColumn('City', lower(dfCityDemo.City)) \\\n",
    "                        .withColumn('State', lower(dfCityDemo.State)) \\\n",
    "                        .withColumn('count', col(\"count\").cast(IntegerType()) ) \n",
    "\n",
    "#Pivots Race to reduce duplicated rows with only race and count being different.\n",
    "dfCityDemo = dfCityDemo.groupBy(\"City\", \"State\", \"Median Age\", \"Male Population\", \"Female Population\", \"Total Population\", \"Number of Veterans\", \"Foreign-born\", \"Average Household Size\", \"State Code\").pivot(\"Race\").sum(\"Count\") \n",
    "\n",
    "#Renames columns to lower case.\n",
    "dfCityDemo = dfCityDemo.withColumnRenamed('City', 'city')\\\n",
    "                        .withColumnRenamed('State', 'state')\\\n",
    "                        .withColumnRenamed('Median Age', 'median_age')\\\n",
    "                        .withColumnRenamed('Male Population', 'male_population') \\\n",
    "                        .withColumnRenamed('Female Population', 'female_population') \\\n",
    "                        .withColumnRenamed('Total Population', 'total_population') \\\n",
    "                        .withColumnRenamed('Number of Veterans', 'number_of_veterans') \\\n",
    "                        .withColumnRenamed('Foreign-born', 'foreign_born') \\\n",
    "                        .withColumnRenamed('Average Household Size', 'average_household_size') \\\n",
    "                        .withColumnRenamed('State Code', 'state_code') \\\n",
    "                        .withColumnRenamed('American Indian and Alaska Native', 'native') \\\n",
    "                        .withColumnRenamed('Asian', 'asian') \\\n",
    "                        .withColumnRenamed('Black or African-American', 'black_african_america') \\\n",
    "                        .withColumnRenamed('Hispanic or Latino', 'hispanic_latino') \\\n",
    "                        .withColumnRenamed('White', 'white') \\\n",
    "\n",
    "#casts columns into appropriate type. \n",
    "dfCityDemo = dfCityDemo.withColumn('median_age', col(\"median_age\").cast(DoubleType())) \\\n",
    "             .withColumn('male_population', col(\"male_population\").cast(IntegerType()) ) \\\n",
    "             .withColumn('female_population', col(\"female_population\").cast(IntegerType()) ) \\\n",
    "             .withColumn('total_population', col(\"total_population\").cast(IntegerType()) ) \\\n",
    "             .withColumn('number_of_veterans', col(\"number_of_veterans\").cast(IntegerType()) ) \\\n",
    "             .withColumn('foreign_born', col(\"foreign_born\").cast(IntegerType()) ) \\\n",
    "             .withColumn('average_household_size', col(\"average_household_size\").cast(DoubleType()) ) \\\n",
    "             .withColumn('native', col(\"native\").cast(IntegerType()) ) \\\n",
    "             .withColumn('asian', col(\"asian\").cast(IntegerType()) ) \\\n",
    "             .withColumn('black_african_america', col(\"black_african_america\").cast(IntegerType()) ) \\\n",
    "             .withColumn('hispanic_latino', col(\"hispanic_latino\").cast(IntegerType()) ) \\\n",
    "             .withColumn('white', col(\"white\").cast(IntegerType()) ) \n",
    "\n",
    "#Verifies that data that exists in Port Code table is inserted only.\n",
    "dfCityDemo.createOrReplaceTempView(\"USADEMOGRAPHICS\")\n",
    "dfCityDemo = spark.sql(\"SELECT * FROM USADEMOGRAPHICS WHERE (state_code, city) in (select statecode, municipality from PORTCODES)\")\n",
    "\n",
    "if config.limitDFtoTen == True:\n",
    "    dfCityDemo = dfCityDemo.limit(10)\n",
    "\n",
    "tableNamesArray.append({'USADEMOGRAPHICS':dfCityDemo})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b815d6-f670-4c67-8325-c2f9198e6825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  ------  IMMIGRATION  TABLE\n",
    "#\n",
    "dfImmigration = spark.read.options(inferSchema='True').parquet(config.immig_file)\n",
    "\n",
    "#Filter dataset to mode 1 which is airplane.\n",
    "dfImmigration = dfImmigration.filter(dfImmigration[\"i94mode\"] == 1)\n",
    "\n",
    "#Cast into Int\n",
    "dfImmigration = dfImmigration \\\n",
    "                .withColumn(\"arrdate\", regexp_replace(dfImmigration[\"arrdate\"], '\\..*$', '').cast(IntegerType()) ) \\\n",
    "                .withColumn(\"depdate\", regexp_replace(dfImmigration[\"depdate\"], '\\..*$', '').cast(IntegerType()) )\n",
    "\n",
    "#Used in the next sql.\n",
    "dfImmigration.createOrReplaceTempView(\"dateTable\")\n",
    "\n",
    "#Adds arrdate and depdate to the year 1960.\n",
    "dfImmigration = spark.sql(\"\"\"\n",
    "                    SELECT T1.*, \n",
    "                    date_add('1960-01-01', arrdate ) as arrival_full,\n",
    "                    day(date_add('1960-01-01', arrdate ) ) as arrival_day,\n",
    "                    month(date_add('1960-01-01', arrdate ) ) as arrival_month,\n",
    "                    year(date_add('1960-01-01', arrdate ) ) as arrival_year,\n",
    "                    date_add('1960-01-01', depdate ) as dep_full,\n",
    "                    day(date_add('1960-01-01', depdate) ) as dep_day,\n",
    "                    month(date_add('1960-01-01', depdate) ) as dep_month,\n",
    "                    year(date_add('1960-01-01', depdate) ) as dep_year\n",
    "                    FROM dateTable T1 \n",
    "                    \"\"\")\n",
    "#Drop not needed columns.\n",
    "dfImmigration = dfImmigration.drop(\"depdate\",\"i94mode\",\"count\", \"admnum\",  \\\n",
    "                                   \"entdepa\",\"entdepd\",\"entdepu\",\"matflag\",\"insnum\", \"i94mon\", \"i94yr\" , \"arrdate\")\n",
    "#Rename Columns\n",
    "dfImmigration = dfImmigration\\\n",
    "                        .withColumnRenamed('i94cit', 'city') \\\n",
    "                        .withColumnRenamed('i94res', 'residence') \\\n",
    "                        .withColumnRenamed('i94port', 'port') \\\n",
    "                        .withColumnRenamed('i94addr', 'address') \\\n",
    "                        .withColumnRenamed('i94bir', 'age') \\\n",
    "                        .withColumnRenamed('i94visa', 'visa') \\\n",
    "\n",
    "#Cast Columns\n",
    "dfImmigration = dfImmigration \\\n",
    "                .withColumn('cicid', col(\"cicid\").cast(IntegerType()) ) \\\n",
    "                .withColumn('city', col(\"city\").cast(IntegerType()) ) \\\n",
    "                .withColumn('residence', col(\"residence\").cast(IntegerType()) ) \\\n",
    "                .withColumn('age', col(\"age\").cast(IntegerType()) ) \\\n",
    "                .withColumn('visa', col(\"visa\").cast(IntegerType()) ) \\\n",
    "                .withColumn('biryear', col(\"biryear\").cast(IntegerType()) ) \\\n",
    "\n",
    "\n",
    "if config.limitDFtoTen == True:\n",
    "    dfImmigration = dfImmigration.limit(10)\n",
    "\n",
    "tableNamesArray.append({'IMMIGRATION':dfImmigration})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee643c7-992c-42e4-b8af-e2d8e092ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes all Dataframes into Redshift.\n",
    "for tables in tableNamesArray:\n",
    "    [[key, value]] = tables.items()\n",
    "    redShiftWrite(value, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fedf1a-ead7-4cbb-bcc0-8ddd259e6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirms data has been inserted into the tables.\n",
    "for tables in tableNamesArray:\n",
    "    [[key, value]] = tables.items()\n",
    "    currentTable = redShiftRead(spark, key)\n",
    "    if currentTable.count() == value.count():\n",
    "        print(\"Count correct on table : \" + key)\n",
    "    else:\n",
    "        print(\"Count incorrect on table : \" + key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
