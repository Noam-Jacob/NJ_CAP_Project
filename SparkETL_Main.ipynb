{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1969d93-83de-4afc-b1fd-33301bffa2c0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, lower, upper, col, trim, year, month, dayofweek, translate, concat, regexp_replace, date_format, date_add, round\n",
    "from pyspark.sql.types import IntegerType, StringType, IntegerType, DoubleType\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "from Functions.redShiftFunctions import redShiftQuery, redShiftWrite, redShiftRead\n",
    "from Functions.loggerObject import createLogger\n",
    "import config\n",
    "\n",
    "#Create logger:\n",
    "logger = createLogger('SparkJob_001', 'logFile_{}_.log'.format(datetime.now().strftime('%Y_%h')) )\n",
    "\n",
    "\n",
    "# REPLACE ACCESS KEY AND SECRET KEY in config.py file!\n",
    "conf = SparkConf() \\\n",
    "       .set(\"spark.hadoop.fs.s3a.access.key\", config.s3aAccessKey) \\\n",
    "       .set(\"spark.hadoop.fs.s3a.secret.key\", config.s3aSecretKey)\\\n",
    "       .set(\"spark.hadoop.fs.s3a.endpoint\", config.s3aEndpoint)\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder.master(\"local[*]\") \\\n",
    "        .appName(\"Main ETL for the Capstone Project\")\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "#Used to collect all dataframes.\n",
    "tableNamesArray = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d07d6f-9b6d-4f44-8dc4-5011833e8fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  ------  PORT LOCATION TABLE -----------\n",
    "try:\n",
    "    \n",
    "    #Read the Portal Codes json file.\n",
    "    textFile = spark.sparkContext.textFile(config.prtlCodes_file)\n",
    "    dfPortCodes = spark.read.json(textFile)\n",
    "\n",
    "    #Remove any rows that do not contain a name and drop duplicates.\n",
    "    dfPortCodes = dfPortCodes.filter(~dfPortCodes.name.contains(\"No PORT\") & ~dfPortCodes.name.contains(\"Collapsed\") )\n",
    "    dfPortCodes = dfPortCodes.drop_duplicates()\n",
    "\n",
    "    #Split the name to columns municipality and statecode.\n",
    "    nameSplit = split(dfPortCodes.name, ',')\n",
    "    dfPortCodes = dfPortCodes.withColumn('municipality', trim (lower( nameSplit.getItem(0) ) ) ) \\\n",
    "                             .withColumn('statecode', trim(upper(nameSplit.getItem(1)))  ) \n",
    "\n",
    "    #Remove any rows without state code. Drop column name as its been split to two new columns.\n",
    "    dfPortCodes = dfPortCodes.filter(~dfPortCodes.statecode.isNull()).drop(\"name\")\n",
    "\n",
    "    dfStateCode = spark.read.options(header=True, delimiter=',', inferSchema='True').csv(config.stateCodes_file)\n",
    "\n",
    "    #Joins Portal Codes and State Codes together to have full state name.\n",
    "    dfPortCodes = dfPortCodes.join(dfStateCode, dfPortCodes.statecode == dfStateCode[\"Alpha code\"], how=\"left\")\n",
    "\n",
    "    #Drops Alpha code which is a duplicate of state code.\n",
    "    dfPortCodes = dfPortCodes.drop(\"Alpha code\").withColumn(\"state\", lower(dfPortCodes.State))\n",
    "\n",
    "    dfPortCodes.createOrReplaceTempView(\"PORTCODES\")\n",
    "\n",
    "    if config.limitDFtoTen:\n",
    "        dfPortCodes = dfPortCodes.limit(10)\n",
    "\n",
    "    #Appends Data frame with Key to be used at the end to insert into Database.\n",
    "    tableNamesArray.append({'PORTCODES':dfPortCodes})\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f'PORT LOCATION Table creations:  {e}')\n",
    "else:\n",
    "    logger.info(f'PORT LOCATION Dataframe size {dfPortCodes.count()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa992e-5e15-4168-a605-4cd263c958d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  ------  AIRPORTS TABLE\n",
    "#\n",
    "\n",
    "dfAirPorts = spark.read.option(\"header\",True).csv(config.airportCodes_file)\n",
    "\n",
    "# Our analysis will be set for US only.\n",
    "dfAirPorts = dfAirPorts.filter( (dfAirPorts.iso_country == 'US') \\\n",
    "                               & (dfAirPorts.type != 'heliport') \\\n",
    "                               & (dfAirPorts.type != 'closed') \\\n",
    "                              )\n",
    "\n",
    "\n",
    "latLongSplit = split(dfAirPorts.coordinates, ',')\n",
    "isoRegSplit = split(dfAirPorts.iso_region, '-')\n",
    "\n",
    "dfAirPorts = dfAirPorts.withColumn('longitude', latLongSplit.getItem(0).cast(DoubleType())) \\\n",
    "        .withColumn('latitude', latLongSplit.getItem(1).cast(DoubleType())) \\\n",
    "        .withColumn('statecode', upper(isoRegSplit.getItem(1))) \\\n",
    "        .withColumn('municipality', lower(col(\"municipality\"))) \\\n",
    "        .withColumn('name', lower(col(\"name\"))) \\\n",
    "        .withColumn('elevation_ft', col(\"elevation_ft\").cast(IntegerType())) \n",
    "\n",
    "\n",
    "#Remove columns continent and iso_country since it is US only.\n",
    "dfAirPorts = dfAirPorts.drop(\"iso_region\", \"continent\", \"coordinates\", \"gps_code\", \"local_code\")\n",
    "\n",
    "if config.limitDFtoTen:\n",
    "    dfAirPorts = dfAirPorts.limit(10)\n",
    "\n",
    "#Append table.\n",
    "tableNamesArray.append({'AIRPORTS':dfAirPorts})\n",
    "\n",
    "dfAirPorts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af6b24-6d1e-4be8-8df8-74f032484a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  ------  USATEMP  TABLE\n",
    "#\n",
    "\n",
    "dfTemps = spark.read.options(header=True, delimiter=',', inferSchema='True').csv(config.globalLandTemp_file)\n",
    "\n",
    "#Filter for US only and removes any rows where Average Temp does not exist.\n",
    "dfTemps = dfTemps.filter( (dfTemps.Country == 'United States') & (dfTemps.AverageTemperature != 'Nan') )\n",
    "\n",
    "#Splits date into year, month, and day of week. Lower cases state to make it easy to join.\n",
    "dfTemps = dfTemps.withColumn('year', year(dfTemps.dt)) \\\n",
    "                 .withColumn('month', month(dfTemps.dt))\\\n",
    "                 .withColumn('dayOfweek', dayofweek(dfTemps.dt))\\\n",
    "                 .withColumn('State', lower (dfTemps.State)) \n",
    "\n",
    "#Create an unique ID for the rows. Rounds Tempature to 3 decimel places.\n",
    "dfTemps = dfTemps.withColumn(\"id\", concat(translate(dfTemps[\"dt\"], \"-\" , \"\"), dfTemps.State )) \\\n",
    "                 .withColumn(\"AverageTemperature\", round(\"AverageTemperature\", 3)) \\\n",
    "                 .withColumn(\"AverageTemperatureUncertainty\", round(\"AverageTemperatureUncertainty\", 3))\n",
    "\n",
    "dfTemps = dfTemps.drop(\"dt\", \"Country\")\n",
    "\n",
    "if config.limitDFtoTen:\n",
    "    dfTemps = dfTemps.limit(10)\n",
    "    \n",
    "#Append table.\n",
    "tableNamesArray.append({'USATEMP':dfTemps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45c85f-de2c-45fe-a4b2-cca6be8e21ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  ------  USADEMOGRAPHICS  TABLE\n",
    "#\n",
    "dfCityDemo = spark.read.options(header=True, delimiter=';').csv(config.usDemograph_file)\n",
    "\n",
    "#Lower cases columns to be used easily when joining.\n",
    "dfCityDemo = dfCityDemo.withColumn('City', lower(dfCityDemo.City)) \\\n",
    "                        .withColumn('State', lower(dfCityDemo.State)) \\\n",
    "                        .withColumn('count', col(\"count\").cast(IntegerType()) ) \n",
    "\n",
    "#Pivots Race to reduce duplicated rows with only race and count being different.\n",
    "dfCityDemo = dfCityDemo.groupBy(\"City\", \"State\", \"Median Age\", \"Male Population\", \"Female Population\", \"Total Population\", \"Number of Veterans\", \"Foreign-born\", \"Average Household Size\", \"State Code\") \\\n",
    "                        .pivot(\"Race\") \\\n",
    "                        .sum(\"Count\") \n",
    "\n",
    "#Renames columns to lower case.\n",
    "#casts columns into appropriate type.\n",
    "dfCityDemo = dfCityDemo.select( \n",
    "                                col('City').alias('city'),\n",
    "                                col('State').alias('state'),\n",
    "                                col('State Code').alias('state_code'), \n",
    "                                col('Median Age').alias('median_age').cast(DoubleType()), \n",
    "                                col('Male Population').alias('male_population').cast(IntegerType()), \n",
    "                                col('Female Population').alias('female_population').cast(IntegerType()), \n",
    "                                col('Total Population').alias('total_population').cast(IntegerType()), \n",
    "                                col('Foreign-born').alias('foreign_born').cast(IntegerType()), \n",
    "                                col('Average Household Size').alias('average_household_size').cast(DoubleType()), \n",
    "                                col('American Indian and Alaska Native').alias('native').cast(IntegerType()), \n",
    "                                col('Asian').alias('asian').cast(IntegerType()),\n",
    "                                col('Black or African-American').alias('black_african_america').cast(IntegerType()), \n",
    "                                col('Hispanic or Latino').alias('hispanic_latino').cast(IntegerType()),\n",
    "                                col('White').alias('white').cast(IntegerType())\n",
    "                                )\n",
    "\n",
    "\n",
    "#Verifies that data that exists in Port Code table is inserted only.\n",
    "dfCityDemo.createOrReplaceTempView(\"USADEMOGRAPHICS\")\n",
    "dfCityDemo = spark.sql(\"SELECT * FROM USADEMOGRAPHICS WHERE (state_code, city) in (select statecode, municipality from PORTCODES)\")\n",
    "\n",
    "if config.limitDFtoTen:\n",
    "    dfCityDemo = dfCityDemo.limit(10)\n",
    "\n",
    "tableNamesArray.append({'USADEMOGRAPHICS':dfCityDemo})\n",
    "\n",
    "dfCityDemo.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b815d6-f670-4c67-8325-c2f9198e6825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  ------  IMMIGRATION  TABLE\n",
    "#\n",
    "dfImmigration = spark.read.options(inferSchema='True').parquet(config.immig_file)\n",
    "\n",
    "#Filter dataset to mode 1 which is airplane.\n",
    "dfImmigration = dfImmigration.filter(dfImmigration[\"i94mode\"] == 1)\n",
    "\n",
    "#Cast into Int\n",
    "dfImmigration = dfImmigration \\\n",
    "                .withColumn(\"arrdate\", regexp_replace(dfImmigration[\"arrdate\"], '\\..*$', '').cast(IntegerType()) ) \\\n",
    "                .withColumn(\"depdate\", regexp_replace(dfImmigration[\"depdate\"], '\\..*$', '').cast(IntegerType()) )\n",
    "\n",
    "#Used in the next sql.\n",
    "dfImmigration.createOrReplaceTempView(\"dateTable\")\n",
    "\n",
    "#Adds arrdate and depdate to the year 1960.\n",
    "dfImmigration = spark.sql(\"\"\"\n",
    "                    SELECT T1.*, \n",
    "                    date_add('1960-01-01', arrdate ) as arrival_full,\n",
    "                    day(date_add('1960-01-01', arrdate ) ) as arrival_day,\n",
    "                    month(date_add('1960-01-01', arrdate ) ) as arrival_month,\n",
    "                    year(date_add('1960-01-01', arrdate ) ) as arrival_year,\n",
    "                    date_add('1960-01-01', depdate ) as dep_full,\n",
    "                    day(date_add('1960-01-01', depdate) ) as dep_day,\n",
    "                    month(date_add('1960-01-01', depdate) ) as dep_month,\n",
    "                    year(date_add('1960-01-01', depdate) ) as dep_year\n",
    "                    FROM dateTable T1 \n",
    "                    \"\"\")\n",
    "\n",
    "\n",
    "dfImmigration = dfImmigration.select(\n",
    "                                    col('cicid').cast(IntegerType()),\n",
    "                                    col('i94cit').alias('city').cast(IntegerType()),\n",
    "                                    col('i94res').alias('residence').cast(IntegerType()),\n",
    "                                    col('i94port').alias('port').cast(IntegerType()),\n",
    "                                    col('i94addr').alias('address').cast(IntegerType()),\n",
    "                                    col('i94bir').alias('age').cast(IntegerType()),\n",
    "                                    col('i94visa').alias('visa').cast(IntegerType()),\n",
    "                                    col('dtadfile'),\n",
    "                                    col('visapost'),\n",
    "                                    col('biryear').cast(IntegerType()),\n",
    "                                    col('dtaddto'), col('gender'), col('airline'), \n",
    "                                    col('fltno'), col('visatype'), col('arrival_full'), \n",
    "                                    col('arrival_day'), col('arrival_month'), col('arrival_year'), \n",
    "                                    col('dep_full'), col('dep_day'), col('dep_month'), col('dep_year')\n",
    "                                    )\n",
    "\n",
    "\n",
    "\n",
    "if config.limitDFtoTen:\n",
    "    dfImmigration = dfImmigration.limit(10)\n",
    "\n",
    "tableNamesArray.append({'IMMIGRATION':dfImmigration})\n",
    "\n",
    "dfImmigration.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee643c7-992c-42e4-b8af-e2d8e092ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes all Dataframes into Redshift.\n",
    "for tables in tableNamesArray:\n",
    "    [[key, value]] = tables.items()\n",
    "    redShiftWrite(value, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fedf1a-ead7-4cbb-bcc0-8ddd259e6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirms data has been inserted into the tables.\n",
    "for tables in tableNamesArray:\n",
    "    [[key, value]] = tables.items()\n",
    "    currentTable = redShiftRead(spark, key)\n",
    "    if currentTable.count() == value.count():\n",
    "        print(\"Count correct on table : \" + key)\n",
    "    else:\n",
    "        print(\"Count incorrect on table : \" + key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
